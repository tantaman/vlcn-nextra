---
description: SQLite isn't it
date: December 18, 2023
draft: true
---

Related: [[reflections]]

> Preface: I've been building with nothing but SQLite for 2 solid years. Apps from [strut.io](https://strut.io), to [linear.app](https://linear.app) clones to music players. I've also bet most of my work on the future of SQLite when it comes to [vlcn.io](https://vlcn.io). SQLite is still the best we have in terms of embedded relational DBs but, all that said, it's a square peg trying to fit a round hole.

[SQLite](https://sqlite.org/) is a bad fit for client side applications. It is designed for 1990's- early 2000's era request-response style and it completely misses the mark when it comes to addressing the needs of today's client side applications.

Let's take a step back. How do today's client side applications make use of client side data stores? Applications like: [Notion](https://www.notion.so/), [Figma](https://figma.com/), Google Docs, Google Maps, WhatsApp, Messenger, Spotify, etc.

# Observation 1: Static Queries, Dynamic Data

> Most databases were built when data was static and queries were dynamic. For applications, most queries are static and the data is dynamic

Applications, other than BI and analytics apps, have relatively static queries but dynamic data.

Take Messenger as an example. The query that shows the user their list of threads is a static one. It is the same query every time. The threads that that query is surfacing, however, are changing all the time. Same with a Google doc. The queries to show its content, comments, edits, viewers, etc. are all static. Each of those datasets, however, changes all the time.

The model of static queries and dynamic data requires a subscription model

```sql
SUBSCRIBE TO SELECT thread.name, group_array(user.status) FROM thread JOIN user ON user.id IN thread.participants;
```

but SQLite only provides a request-response model for queries.

Since SQLite only support request-response, using it to implement a view that updates whenever the underlying data changes requires one or more of:

1. Polling SQLite
2. Relying on the shaky [data change notification callbacks](https://www.sqlite.org/c3ref/update_hook.html) provided by SQLite
3. Creating a separate data model (e.g., a View Model or a Domain Model) on top of SQLite that handles reactivity 

## Polling

Polling isn't ideal since:

1. It introduces latency into the system. Responding to a write is only as quick as the polling interval.
2. Wastes resources. The data may not have changed between polls.
3. Wastes resources. Re-runs a full query rather than only computing a difference.

This doesn't mean that polling _can't_ work. Maybe you have few enough objects returned by all your queries that you can take the hit of re-running them 30-60 times a second. Or maybe only a few things need to update in the next frame, everything else can wait seconds. While this may be the case today, it'll also need to be the case tomorrow.

## Data Change Notifications

SQLite does provide [data change notification callbacks](https://www.sqlite.org/c3ref/update_hook.html) but they're severely limited.

1. They do not work on `without rowid` tables
2. They do not provide notifications if other connections make a write
3. They do not provide notifications if other processes make a write
4. They only tell you the `rowid`, `table`, `db`, and `operation` of the change. It is your job to figure out what queries that would impact.
5. They don't contain the data that changed meaning you need to re-query to get the new values

The data change notification callbacks are what Vulcan-web currently use to provide reactivity (as well as other projects like [GRDB](https://github.com/groue/GRDB.swift)). These projects show that these limitations can be worked around if you're clever (by combining sqlite's byte code extension, pre_update hook, commit_hook, authorizer callbacks and more) but you run into another wall.

The solutions:

1. Are not incremental. They require re-running the entire query if the data it used changed.
2. Over-invalidate. Other than point queries (`SELECT FROM foo WHERE id = ?`), you'll need to invalidate and re-run all queries that used any table that was written.

## Separate Data Model 

A final option is building out a separate data model on top of the database. Something where you read and write to objects that exist in-memory and later persist them to the database. This is today's status quo. Think things like CoreData, MobX + Persistence, Room Persistence Library.

Benefits:

1. Read are as fast as working with memory (after initial hydration)
2. Writes are as fast as working with memory (when persistence is done lazily)
3. The libraries include fine grained subscriptions to objects, object properties and collections of objects.

These benefits come with huge downsides as in-memory model is effectively building a database outside of the database.

Downsides:

1. Increased complexity. Must deal with the DB idiosyncrasies as well as the ORM / in-memory model's. The in-memory model adds a whole new layer that requires mapping results to/from.
2. Transactions I. Many of these solutions do not provide an ability to update the in-memory model transactionally, leading to observation of stale state.
3. Transactions II. If many writes are made to the in-memory model then the lazy persist fails, what can be done? Can the in-memory model be rolled back?
4. Invariants. The DB will have foreign key constraints, check constraints, and unique indices. For the in-memory model to always succeed in persisting to the DB, these constraints must be replicated and checked there too.
5. Concurrency. the database will have a defined concurrent model to deal with concurrent updates. Now that needs to be replicated in the in-memory model or concurrency must be forbidden.
6. Other processes. The in-model can not observe writes made by other processes and coordinating processes must be eliminated else risk model & db divergence.
7. Writes may no longer go directly to the DB. Allowing someone to bypass the in-memory model to issue a write will cause the DB and in-memory model to fall out of sync. Trying to subscribe the in-memory model to the DB to fix this lands us back to square 1.
8. Limited subscription primitives. The in-memory model can only be subscribed to on a collection of key-value basis. No subscribing to other styles of queries.

But [don't just take my word for it](https://www.scattered-thoughts.net/writing/against-sql/#the-application-layer).

It is a shock that the status quo has been to use this fraught approach for so long.

Rather than creating a db outside the db, applications should be able to code directly against the database and let the database manage everything it is good at:

- atomicity
- consistency
- isolation & concurrency
- durability
- indexing
- querying

# Observation 2: UIs Present De-normalized Data

Data should always be stored in a normalized form. It ensures there is only ever one copy of a thing and that inconsistent states do not occur. The relational model also ensures that the applications built on top can always access the data in a way that makes sense to them. Contrast with the network or graph models which require knowing specific paths to objects in order to load them and create a tight coupling between how data is laid out and how it is accessed.

I can't defend the relational model any better than it was first defended in 1970 and reading the original rationale for the creation of relational DBs, which have dominated the market ever since, is always illuminating: https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf

Once the data starts to be used, however, it is almost always used in a de-normalized way.

- A presentation editor like [strut.io](https://strut.io) is going to show Slide Decks, Slides, and Components as nested objects not as relations
- A music player is going to join `artist`, `album`, `track`, `playlist` tables to create a single track list view
- A task app like (linear)[linear.app] will join extra data source to enrich a task or task list.

Given:

1. Apps have many static queries
2. These queries fetch de-normalized data

An embedded DB for client side applications should have first class support for incrementally maintained views. There should be no need to re-run the set of joins required to produce a view every time the view is requested.

Taking this a step further -- an embedded DB could even preclude the developer from having to define indices. If the views and queries are known ahead of time (which they are), a build process can create the correct indices to serve the needs of the application.

SQLite, unfortunately, has no support for incrementally maintained views just as it has limited support for subscriptions. One could maintain sets of triggers to manually maintain incremental views but this isn't ideal.

> The counter-argument is that: SQLite, being embedded, [makes many small queries efficient](https://www.sqlite.org/np1queryprob.html) which obviates the need for joins. While this is true, it doesn't completely remove the need for them. In addition, the cost of traversing `WASM<->JS` boundaries in the web platform, makes many small queries much less attractive. Finally, being able to see the full data needs of a UI tree as a single query opens up many new avenues in terms of data syncing and collaboration.

# Observation 3: Write Priorities

Not all writes are created equal. Some writes, like those initiated by a user event, need to be responded to immediately. Other writes, such as a background import of some data, can be given a lower priority.

An example of the latter case is a music app. While importing user tracks, much manual care had to be taken to schedule these writes such that they would not impact read queries that were servicing the UI.

Along the same lines as write priorities, not all writes need to be immediately durable. To give the user a faster experience, it may be preferable to non-durably write their update in order to reflect back changes more quickly.

The inability to selectively relax durability is a performance bottleneck when it comes to responding to writes as we're limited [by the transaction throughput of writing to disk](https://www.sqlite.org/faq.html#q19). SQLite is designed to handle bulk updates well by amortizing the cost of writing to disk over all the statements in a transaction. It is not designed to handle the many small updates you'd get in most applications. E.g., typing characters into a doc, dragging shapes in figma, scrubbing the playback state over a video or song.

# Observation 4: Collaboration

The world of client side applications is not what it was two decades ago when SQLite was created. In the 90's and early 2000's a client side application only had to concern itself with the computer it was running on. Fastforward a bit, everyone has many devices and expects their data to be available on all of those devices.

# Problem: Language Barrier

Application developers would rather query their data in a language that more closely integrates and embeds into their host language. SQL is also an opaque barrier that hides many useful features of the DB. E.g., the query planner or direct index access.

I don't

- Native Language
- GraphQL?
- SQL and types
- SQL and composition
- SQL and moated language - https://www.scattered-thoughts.net/writing/against-sql/
- No common prepared statement cache
- 

# Problem: The Browser Platform

# Ideal

- Code to the DB.

# What Now?

For me? I'm still figuring it out. Maybe I can re-work SQLite enough to shore up these problems in a fork. Maybe it requires something new.

---

- commits
- durability
- concurrency
- invariants
- unidirectional 

---

This means that to support the use case of dynamic data you need to implement some sort of polling or manually invalidate and re-run entire queries.

What you need to support this is queries to which you can subscribe. The model of 

SQLite has minimal support for this setup.


- Normalized
- To de-Normalized
- Index selection and creation
- Query layer / lack of SDK
- Collaboration
- Write prioriteis


What are all of the knock on effects of this?


- Reactivity
- Intent -> SQL -> Parse -> Plan -> Execute -> Finally deliver
- Write priorities. Single writer.
- Incremental View Maintenance
- Collaboration


Re-running queries is especially problematic on the web platform.

----

It's design follows too much in the footsteps of client-server DBs and it doesn't leverage its unique position as an embedded database as well as it could.

You can get very clever and work around all of these limitations. E.g., by watching the DB files (WAL, Journal, SHMem, main DB file) to understand when other processes write, by using a single write connection or coordinating write connections, by inspecting the byte code of read queries to understand all the tables they use, by hacking together `authorizer` and `pre_update` hooks to understand columns used and written. But at the end of the day, it is still a far cry from incremental subscriptions.

# Lessons Learned

Research more? lol


Messenger: https://softwareengineeringdaily.com/2020/03/31/facebook-messenger-engineering-with-mohsen-agsen/
Riffle: https://riffle.systems/



Or, a tldr from [here](https://www.scattered-thoughts.net/writing/against-sql/):

> The relational model is great:
> - A shared universal data model allows cooperation between programs written in many different languages, running on different > machines and with different lifespans.
> - Normalization allows updating data without worrying about forgetting to update derived data.
> - Physical data independence allows changing data-structures and query plans without having to change all of your queries.
> - Declarative constraints clearly communicate application invariants and are automatically enforced.
> - Unlike imperative languages, relational query languages don't have false data dependencies created by loop counters and aliasable pointers. This makes relational languages:
>    - A good match for modern machines. Data can be rearranged for more compact layouts, even automatic compression. Operations can be reordered for high cache locality, pipeline-friendly hot loops, simd etc.
>    - Amenable to automatic parallelization.
>    - Amenable to incremental maintenance.



>  But in the database world there is a missing step between demos on toy relational algebras and handling the enormity of SQL, down which most compelling research quietly plummets. Bringing anything novel to a usable level requires a substantial investment of time and money that most researchers simply don't have.

More on this idea here: https://riffle.systems/essays/prelude/

https://github.com/EvgSkv/logica
https://github.com/tonsky/datascript
